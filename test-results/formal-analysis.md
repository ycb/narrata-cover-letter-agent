# Formal Model Analysis with SQS Metric

**Date:** 2025-10-19  
**Methodology:** Speed√óQuality Score (SQS) = (Coverage% / Latency_s) √ó 10

---

## üìä Coverage Analysis (vs Expected Signal Density)

### Story Coverage by Quality Tier

| Model | Gold (P01-P03) | Medium (P04, P08-P09) | Med-Low (P05-P06) | High (P07, P10) | **Overall** |
|-------|----------------|----------------------|-------------------|-----------------|-------------|
| gpt-3.5-turbo | 87% | 50% | 59% | 47% | **64%** |
| gpt-4o-mini | 96% | 67% | 67% | 54% | **72%** |
| gpt-4o | 96% | 67% | 67% | 54% | **72%** |

### Metrics Coverage by Quality Tier

| Model | Gold (P01-P03) | Medium (P04, P08-P09) | Med-Low (P05-P06) | High (P07, P10) | **Overall** |
|-------|----------------|----------------------|-------------------|-----------------|-------------|
| gpt-3.5-turbo | 94% | 46% | 44% | 46% | **61%** |
| gpt-4o-mini | 89% | 67% | 67% | 50% | **70%** |
| gpt-4o | 94% | 40% | 56% | 50% | **65%** |

**Key Insight:** gpt-4o-mini has best **balanced extraction** (72% stories, 70% metrics)

---

## üéØ Speed√óQuality Score (SQS)

Formula: `SQS = (Coverage% / Latency_s) √ó 10`

### Overall SQS (Combined Story + Metric Coverage)

| Model | Story Coverage | Metric Coverage | Combined Coverage | Latency (s) | **SQS** | Rank |
|-------|----------------|-----------------|-------------------|-------------|---------|------|
| gpt-3.5-turbo | 64% | 61% | **62.5%** | 6.3 | **99** | ü•á |
| gpt-4o-mini | 72% | 70% | **71.0%** | 10.6 | **67** | ü•à |
| gpt-4o | 72% | 65% | **68.5%** | 7.7 | **89** | ü•â |

**Surprising Result:** gpt-3.5-turbo wins on SQS due to speed, despite lower quality!

---

## üìà Tier-Specific SQS (Where Each Model Shines)

### Gold Tier SQS (High Signal Density)

| Model | Coverage | Latency (s) | SQS | Best For |
|-------|----------|-------------|-----|----------|
| gpt-3.5-turbo | 90.5% | 10.1 | **90** | - |
| **gpt-4o-mini** | **92.5%** | 13.6 | **68** | üèÜ Gold profiles |
| gpt-4o | 95.0% | 10.5 | **90** | - |

### Medium-Low Tier SQS (Sparse Signal)

| Model | Coverage | Latency (s) | SQS | Best For |
|-------|----------|-------------|-----|----------|
| **gpt-3.5-turbo** | **51.5%** | 4.3 | **120** | üèÜ Sparse profiles |
| gpt-4o-mini | 67.0% | 8.3 | **81** | - |
| gpt-4o | 61.5% | 6.3 | **98** | - |

**Strategic Insight:** 
- **gpt-3.5-turbo:** Best for sparse, early-career profiles (fast extraction of limited signal)
- **gpt-4o-mini:** Best for rich, experienced profiles (thorough extraction worth the wait)
- **gpt-4o:** Expensive middle ground (not worth 26x cost vs gpt-4o-mini)

---

## üé≠ Precision Analysis: Stories vs Metrics

| Model | Story Precision | Metric Precision | Delta | Diagnosis |
|-------|-----------------|------------------|-------|-----------|
| gpt-3.5-turbo | 64% | 61% | -3% | Balanced extraction |
| gpt-4o-mini | 72% | 70% | -2% | **Most balanced** ‚úÖ |
| gpt-4o | 72% | 65% | -7% | Poor metric extraction |

**Finding:** gpt-4o-mini has the most **consistent extraction** across stories and metrics.

---

## üí° Recommendations by Use Case

### For Onboarding (One-Time, User Watching)
**Winner:** gpt-4o-mini
- **Coverage:** 71% (best balanced)
- **Latency:** 10.6s (acceptable for onboarding)
- **User Perception:** With streaming, feels like 3-5s
- **Cost:** $0.0006 per user

**Why not gpt-3.5-turbo?**
- 9% lower coverage (62.5% vs 71%)
- 4.3s faster, but users won't notice with streaming
- Slightly worse on Gold profiles (90.5% vs 92.5%)

### For Background Processing (Async, No User Wait)
**Winner:** gpt-4o-mini
- Same reasoning, no latency penalty since user isn't waiting

### For Real-Time Chat/Query (Low Latency Critical)
**Winner:** gpt-3.5-turbo
- 40% faster (6.3s vs 10.6s)
- SQS of 99 (best overall)
- Acceptable 62.5% coverage for real-time use

---

## üî¨ Production Test Plan

### Test P01 with gpt-4o-mini (Detailed Logging)

**Expected Results:**
- Latency: ~13s (not 98s)
- Retries: 0 (with proper token calc)
- Token usage: ~2,500 input, ~1,200 output

**Log Points:**
1. Request start timestamp
2. Token count before call
3. OpenAI API latency
4. Retry attempts (should be 0)
5. Response parsing time
6. Database save time
7. Total end-to-end time

**If Confirmed (13s, no retries):**
- ‚úÖ Model choice validated
- ‚úÖ Production 98s is a bug (retries, blocking, or network)
- ‚úÖ Streaming + progress bar will make 13s feel like 3-5s

**If NOT Confirmed (60s+, retries):**
- üîç Investigate token calculation in production
- üîç Check for network/proxy issues
- üîç Review OpenAI API tier/rate limits

---

## üìä Final Scoring Matrix

| Model | Coverage | Latency | Cost | SQS | Best Use Case |
|-------|----------|---------|------|-----|---------------|
| gpt-3.5-turbo | 62.5% | 6.3s | $0.0016 | **99** | Real-time, sparse profiles |
| **gpt-4o-mini** | **71.0%** | 10.6s | **$0.0006** | **67** | **Onboarding, rich profiles** üèÜ |
| gpt-4o | 68.5% | 7.7s | $0.0167 | 89 | Not recommended (26x cost) |

---

**Decision:** Use **gpt-4o-mini** for onboarding with streaming UI.

**Next Steps:**
1. Test P01 import with detailed logging
2. Implement streaming progress updates
3. Measure perceived latency (target: <5s perceived)
4. Monitor production metrics for 98s regression



